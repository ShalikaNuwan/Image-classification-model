{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pytorch\n",
    "import torchvision\n",
    "import torchvision . transforms as transforms\n",
    "# Define data transformations ( optional , butrecommended )\n",
    "transform = transforms . Compose ([ transforms . ToTensor () ,transforms . Normalize ((0.5 , 0.5 , 0.5) , (0.5 , 0.5 ,0.5) ) ])\n",
    "# Load the training dataset\n",
    "\n",
    "    \n",
    "# train_dataset = torchvision . datasets . CIFAR10 ( root =r'D:\\semester 5\\Pattern recognition\\Assignment 3\\assignment 3', train = True , transform = transform , download =True )\n",
    "# # Load the testing dataset\n",
    "# test_dataset = torchvision . datasets . CIFAR10 ( root =r'D:\\semester 5\\Pattern recognition\\Assignment 3\\assignment 3 ', train = False , transform = transform , download =True )\n",
    "# # for keras\n",
    "from keras . datasets import cifar10\n",
    "# Load the CIFAR -10 dataset\n",
    "( train_images , train_labels ) , ( test_images ,test_labels ) = cifar10 . load_data ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "images = np.concatenate((train_images, test_images), axis=0)\n",
    "labels = np.concatenate((train_labels,test_labels),axis=0)\n",
    "\n",
    "number_of_train_images=int(images.shape[0]*0.6)\n",
    "number_of_test_images=int(images.shape[0]*0.2)\n",
    "number_of_validation_images=int(images.shape[0]*0.2)\n",
    "train_images,validation_images,test_images=np.split(images,[number_of_train_images,number_of_train_images+number_of_validation_images])\n",
    "train_labels,validation_labels,test_labels=np.split(labels,[number_of_train_images,number_of_train_images+number_of_validation_images])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the new input shape you want (e.g., (32, 32, 3))\n",
    "new_input_shape = (32, 32, 3)\n",
    "\n",
    "# Create a new input layer with the desired shape\n",
    "new_input = Input(shape=new_input_shape)\n",
    "\n",
    "# Load the pre-trained ResNet-50 model with the new input layer\n",
    "model = ResNet50(weights='imagenet', include_top=True, input_tensor=new_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model.fc = nn.Linear(in_features=64, out_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except the last one (custom classifier)\n",
    "for layer in model.layers[:-1]:  \n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1125/1125 [==============================] - 153s 133ms/step - loss: 1.0182 - accuracy: 0.6722 - val_loss: 1.8368 - val_accuracy: 0.5698\n",
      "Epoch 2/20\n",
      "1125/1125 [==============================] - 142s 126ms/step - loss: 1.0037 - accuracy: 0.6764 - val_loss: 1.8366 - val_accuracy: 0.5596\n",
      "Epoch 3/20\n",
      "1125/1125 [==============================] - 150s 134ms/step - loss: 1.0072 - accuracy: 0.6745 - val_loss: 1.8799 - val_accuracy: 0.5588\n",
      "Epoch 4/20\n",
      "1125/1125 [==============================] - 154s 137ms/step - loss: 1.0027 - accuracy: 0.6772 - val_loss: 1.8104 - val_accuracy: 0.5673\n",
      "Epoch 5/20\n",
      "1125/1125 [==============================] - 158s 140ms/step - loss: 1.0079 - accuracy: 0.6743 - val_loss: 1.8459 - val_accuracy: 0.5642\n",
      "Epoch 6/20\n",
      "1125/1125 [==============================] - 151s 134ms/step - loss: 1.0008 - accuracy: 0.6792 - val_loss: 1.8601 - val_accuracy: 0.5676\n",
      "Epoch 7/20\n",
      "1125/1125 [==============================] - 154s 137ms/step - loss: 1.0065 - accuracy: 0.6763 - val_loss: 1.8249 - val_accuracy: 0.5664\n",
      "Epoch 8/20\n",
      "1125/1125 [==============================] - 158s 140ms/step - loss: 0.9938 - accuracy: 0.6787 - val_loss: 1.8919 - val_accuracy: 0.5583\n",
      "Epoch 9/20\n",
      "1125/1125 [==============================] - 147s 131ms/step - loss: 0.9961 - accuracy: 0.6817 - val_loss: 1.8382 - val_accuracy: 0.5735\n",
      "Epoch 10/20\n",
      "1125/1125 [==============================] - 134s 120ms/step - loss: 0.9880 - accuracy: 0.6819 - val_loss: 1.9632 - val_accuracy: 0.5516\n",
      "Epoch 11/20\n",
      "1125/1125 [==============================] - 155s 138ms/step - loss: 1.0038 - accuracy: 0.6777 - val_loss: 1.9398 - val_accuracy: 0.5628\n",
      "Epoch 12/20\n",
      "1125/1125 [==============================] - 153s 136ms/step - loss: 0.9942 - accuracy: 0.6817 - val_loss: 1.8827 - val_accuracy: 0.5673\n",
      "Epoch 13/20\n",
      "1125/1125 [==============================] - 155s 138ms/step - loss: 0.9943 - accuracy: 0.6817 - val_loss: 1.9541 - val_accuracy: 0.5543\n",
      "Epoch 14/20\n",
      "1125/1125 [==============================] - 241s 214ms/step - loss: 0.9974 - accuracy: 0.6813 - val_loss: 1.9700 - val_accuracy: 0.5555\n",
      "Epoch 15/20\n",
      "1125/1125 [==============================] - 137s 122ms/step - loss: 0.9936 - accuracy: 0.6818 - val_loss: 1.9748 - val_accuracy: 0.5543\n",
      "Epoch 16/20\n",
      "1125/1125 [==============================] - 147s 131ms/step - loss: 0.9962 - accuracy: 0.6776 - val_loss: 1.9570 - val_accuracy: 0.5643\n",
      "Epoch 17/20\n",
      "1125/1125 [==============================] - 137s 122ms/step - loss: 0.9851 - accuracy: 0.6854 - val_loss: 1.9787 - val_accuracy: 0.5527\n",
      "Epoch 18/20\n",
      "1125/1125 [==============================] - 265s 236ms/step - loss: 0.9848 - accuracy: 0.6830 - val_loss: 1.9567 - val_accuracy: 0.5639\n",
      "Epoch 19/20\n",
      "1125/1125 [==============================] - 150s 133ms/step - loss: 0.9858 - accuracy: 0.6817 - val_loss: 1.9695 - val_accuracy: 0.5657\n",
      "Epoch 20/20\n",
      "1125/1125 [==============================] - 127s 113ms/step - loss: 1.0004 - accuracy: 0.6803 - val_loss: 1.9647 - val_accuracy: 0.5629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x164a509c400>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, validation_data=(validation_images, validation_labels), epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze previous layers \n",
    "for layer in model.layers[:-1]:  \n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1125/1125 [==============================] - 119s 105ms/step - loss: 0.9917 - accuracy: 0.6809 - val_loss: 2.0203 - val_accuracy: 0.5578\n",
      "Epoch 2/20\n",
      "1125/1125 [==============================] - 123s 110ms/step - loss: 0.9987 - accuracy: 0.6831 - val_loss: 2.0069 - val_accuracy: 0.5570\n",
      "Epoch 3/20\n",
      "1125/1125 [==============================] - 124s 111ms/step - loss: 0.9873 - accuracy: 0.6871 - val_loss: 2.0575 - val_accuracy: 0.5494\n",
      "Epoch 4/20\n",
      "1125/1125 [==============================] - 123s 109ms/step - loss: 0.9884 - accuracy: 0.6867 - val_loss: 2.0017 - val_accuracy: 0.5597\n",
      "Epoch 5/20\n",
      "1125/1125 [==============================] - 122s 108ms/step - loss: 0.9937 - accuracy: 0.6820 - val_loss: 2.0026 - val_accuracy: 0.5564\n",
      "Epoch 6/20\n",
      "1125/1125 [==============================] - 128s 114ms/step - loss: 0.9903 - accuracy: 0.6827 - val_loss: 1.9907 - val_accuracy: 0.5669\n",
      "Epoch 7/20\n",
      "1125/1125 [==============================] - 125s 112ms/step - loss: 0.9788 - accuracy: 0.6861 - val_loss: 2.0685 - val_accuracy: 0.5523\n",
      "Epoch 8/20\n",
      "1125/1125 [==============================] - 126s 112ms/step - loss: 0.9900 - accuracy: 0.6840 - val_loss: 2.1093 - val_accuracy: 0.5515\n",
      "Epoch 9/20\n",
      "1125/1125 [==============================] - 128s 114ms/step - loss: 0.9851 - accuracy: 0.6852 - val_loss: 2.1119 - val_accuracy: 0.5523\n",
      "Epoch 10/20\n",
      "1125/1125 [==============================] - 127s 113ms/step - loss: 0.9882 - accuracy: 0.6852 - val_loss: 2.1298 - val_accuracy: 0.5490\n",
      "Epoch 11/20\n",
      "1125/1125 [==============================] - 130s 116ms/step - loss: 0.9940 - accuracy: 0.6849 - val_loss: 2.0865 - val_accuracy: 0.5547\n",
      "Epoch 12/20\n",
      "1125/1125 [==============================] - 128s 114ms/step - loss: 0.9856 - accuracy: 0.6877 - val_loss: 2.0804 - val_accuracy: 0.5552\n",
      "Epoch 13/20\n",
      "1125/1125 [==============================] - 132s 117ms/step - loss: 0.9945 - accuracy: 0.6840 - val_loss: 2.1380 - val_accuracy: 0.5508\n",
      "Epoch 14/20\n",
      "1125/1125 [==============================] - 127s 113ms/step - loss: 0.9854 - accuracy: 0.6880 - val_loss: 2.1153 - val_accuracy: 0.5572\n",
      "Epoch 15/20\n",
      "1125/1125 [==============================] - 139s 124ms/step - loss: 0.9813 - accuracy: 0.6862 - val_loss: 2.1643 - val_accuracy: 0.5531\n",
      "Epoch 16/20\n",
      "1125/1125 [==============================] - 128s 114ms/step - loss: 0.9845 - accuracy: 0.6869 - val_loss: 2.1938 - val_accuracy: 0.5451\n",
      "Epoch 17/20\n",
      "1125/1125 [==============================] - 126s 112ms/step - loss: 0.9872 - accuracy: 0.6894 - val_loss: 2.1392 - val_accuracy: 0.5518\n",
      "Epoch 18/20\n",
      "1125/1125 [==============================] - 127s 113ms/step - loss: 0.9845 - accuracy: 0.6867 - val_loss: 2.1368 - val_accuracy: 0.5570\n",
      "Epoch 19/20\n",
      "1125/1125 [==============================] - 127s 113ms/step - loss: 0.9818 - accuracy: 0.6889 - val_loss: 2.0829 - val_accuracy: 0.5502\n",
      "Epoch 20/20\n",
      "1125/1125 [==============================] - 126s 112ms/step - loss: 0.9808 - accuracy: 0.6894 - val_loss: 2.1223 - val_accuracy: 0.5542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x164a71ee920>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again train the model using dataset\n",
    "model.fit(train_images, train_labels, validation_data=(validation_images, validation_labels), epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
